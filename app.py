import streamlit as st
import openai
import google.generativeai as genai

# -----------------------------------------------------------
# 1. ê¸°ë³¸ ì„¤ì • & ì„¸ì…˜(ê¸°ì–µ ì €ì¥ì†Œ) ì´ˆê¸°í™”
# -----------------------------------------------------------
st.set_page_config(page_title="MJP: Interactive Research Partner", layout="wide")

# [í•µì‹¬] ë…¼ë¬¸ì˜ ê° ì±•í„° ë‚´ìš©ì„ ë”°ë¡œë”°ë¡œ ê¸°ì–µí•˜ëŠ” ì €ì¥ì†Œ
if 'paper_sections' not in st.session_state:
    st.session_state['paper_sections'] = {
        "ì„œë¡ ": "",
        "ì´ë¡ ì  ë°°ê²½": "",
        "ì—°êµ¬ ë°©ë²•": "",
        "ê²°ê³¼": "",
        "ë…¼ì˜": ""
    }

# ì—°êµ¬ ì„¤ê³„ ë°ì´í„° ì €ì¥ì†Œ
if 'research_context' not in st.session_state:
    st.session_state['research_context'] = {
        'topic': '',
        'variables': '',
        'method': '',
        'references': ''
    }

# ì±„íŒ… ê¸°ë¡ ì €ì¥ì†Œ
if "messages" not in st.session_state:
    st.session_state.messages = []

# -----------------------------------------------------------
# 2. ì‚¬ì´ë“œë°”: ë¡œê·¸ì¸ & ì„¤ì •
# -----------------------------------------------------------
with st.sidebar:
    st.header("ğŸ” ì—°êµ¬ì‹¤ ì…ì¥")
    code = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
    if code not in st.secrets["ACCESS_CODES"]:
        st.warning("ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.stop()
    
    st.success("System Online")
    
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ìš© ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()

# API í‚¤ ì—°ê²°
openai.api_key = st.secrets["OPENAI_API_KEY"]
genai.configure(api_key=st.secrets["GEMINI_API_KEY"])

# -----------------------------------------------------------
# 3. AI ë‘ë‡Œ (ê¸°ëŠ¥ ì •ì˜)
# -----------------------------------------------------------

def consult_variables(topic):
    prompt = f"ì£¼ì œ '{topic}'ì— ì í•©í•œ ë…ë¦½, ì¢…ì†, ì¡°ì ˆ/ë§¤ê°œ ë³€ì¸ êµ¬ì¡°ë¥¼ 3ê°œ ì œì•ˆí•´ì¤˜."
    response = openai.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}])
    return response.choices[0].message.content

def design_methodology(vars_text):
    prompt = f"ë³€ì¸ '{vars_text}'ì„ ì¸¡ì •í•  ì²™ë„ì™€ í†µê³„ ë¶„ì„ ë°©ë²•ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•´ì¤˜."
    response = openai.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}])
    return response.choices[0].message.content

def search_literature(topic, vars_text):
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        prompt = f"ì£¼ì œ: {topic}, ë³€ì¸: {vars_text}. ê´€ë ¨ ìµœì‹  ì„ í–‰ ì—°êµ¬(2020-2026)ì™€ í•µì‹¬ ì´ë¡  ìš”ì•½."
        return model.generate_content(prompt).text
    except:
        return "ê²€ìƒ‰ ì˜¤ë¥˜. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."

def write_paper_final(section, context_data):
    # [ìˆ˜ì •] ë” êµ¬ì²´ì ì´ê³  ë…¼ë¦¬ì ì¸ ê¸€ì“°ê¸°ë¥¼ ìœ„í•œ ì§€ì‹œ ê°•í™”
    prompt = f"""
    [ì—­í• ]: ë‹¹ì‹ ì€ ë§¤ìš° ë¹„íŒì ì´ê³  ë…¼ë¦¬ì ì¸ ì‹¬ë¦¬í•™ ë…¼ë¬¸ ì‘ì„±ìì…ë‹ˆë‹¤.
    [ì‘ì—…]: '{section}' ì±•í„° ì´ˆì•ˆ ì‘ì„±.
    [ê·¼ê±° ë°ì´í„°]: {context_data}
    
    [í•„ìˆ˜ ì§€ì¹¨]:
    1. ì¶”ìƒì ì¸ í‘œí˜„(ì˜ˆ: 'ì˜í–¥ì„ ë¯¸ì³¤ë‹¤')ì„ ì§€ì–‘í•˜ê³ , êµ¬ì²´ì ì¸ ê¸°ì œë‚˜ ë…¼ë¦¬ë¥¼ ì„œìˆ í•  ê²ƒ.
    2. ë¬¸ì¥ ê°„ì˜ ì¸ê³¼ê´€ê³„ê°€ ëª…í™•í•´ì•¼ í•¨. ë¹„ì•½ì´ ì—†ë„ë¡ ì£¼ì˜í•  ê²ƒ.
    3. APA ìŠ¤íƒ€ì¼ì„ ì² ì €íˆ ì¤€ìˆ˜í•  ê²ƒ.
    """
    response = openai.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}])
    return response.choices[0].message.content

def organize_references_apa(raw_text):
    prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì„ ì¶”ì¶œí•˜ì—¬ APA 7íŒ ì–‘ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì•ŒíŒŒë²³/ê°€ë‚˜ë‹¤ ìˆœ ì •ë ¬í•´ì¤˜:\n{raw_text}"
    response = openai.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}])
    return response.choices[0].message.content

# -----------------------------------------------------------
# 4. í™”ë©´ êµ¬ì„± (íƒ­ 6ê°œ)
# -----------------------------------------------------------
st.title("ğŸ“ MJP: ëŒ€í™”í˜• ë…¼ë¬¸ ì‘ì„± ì‹œìŠ¤í…œ V2")

tabs = st.tabs(["1. ë³€ì¸", "2. ë°©ë²•", "3. ê²€ìƒ‰", "4. ë³¸ë¬¸ ì‘ì„±(ì €ì¥)", "5. ì°¸ê³ ë¬¸í—Œ", "ğŸ’¬ 6. AI í”¼ë“œë°±"])

# [Tab 1~3] ì„¤ì • ë‹¨ê³„
with tabs[0]:
    topic = st.text_input("ì—°êµ¬ ì£¼ì œ")
    if st.button("ë³€ì¸ ì œì•ˆ"):
        st.markdown(consult_variables(topic))
        st.session_state['research_context']['topic'] = topic
    final_vars = st.text_area("ë³€ì¸ í™•ì •", key="v_input")
    if st.button("ë³€ì¸ ì €ì¥"): st.session_state['research_context']['variables'] = final_vars

with tabs[1]:
    if st.button("ë°©ë²•ë¡  ì œì•ˆ"): st.markdown(design_methodology(st.session_state['research_context']['variables']))
    final_method = st.text_area("ë°©ë²•ë¡  í™•ì •", key="m_input")
    if st.button("ë°©ë²• ì €ì¥"): st.session_state['research_context']['method'] = final_method

with tabs[2]:
    if st.button("Gemini ê²€ìƒ‰"):
        refs = search_literature(st.session_state['research_context']['topic'], st.session_state['research_context']['variables'])
        st.session_state['research_context']['references'] = refs
        st.text_area("ê²€ìƒ‰ ê²°ê³¼", refs)

# -----------------------------------------------------------
# [Tab 4] ë³¸ë¬¸ ì‘ì„± (ì—¬ê¸°ê°€ ë¯¼ì£¼ë‹˜ ìš”ì²­ëŒ€ë¡œ ëŒ€í­ ìˆ˜ì •ë¨!)
# -----------------------------------------------------------
with tabs[3]:
    st.header("âœï¸ 4ë‹¨ê³„: ë³¸ë¬¸ ì‘ì„± (ì±•í„°ë³„ ë…ë¦½ ì €ì¥)")
    
    # 1. ì‘ì„±í•  ì±•í„° ì„ íƒ
    target_section = st.selectbox("ì‘ì„±/í¸ì§‘í•  ì±•í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”", list(st.session_state['paper_sections'].keys()))
    
    col_a, col_b = st.columns([1, 5])
    
    # 2. AI ì´ˆì•ˆ ìƒì„± ë²„íŠ¼
    with col_a:
        if st.button(f"ğŸ¤– AI ì´ˆì•ˆ ìƒì„±"):
            with st.spinner(f"{target_section} ì‘ì„± ì¤‘..."):
                # ê²€ìƒ‰ëœ ìë£Œê°€ ì—†ìœ¼ë©´ ê²½ê³ 
                ref_data = st.session_state['research_context']['references']
                if not ref_data:
                    st.warning("3ë‹¨ê³„ ê²€ìƒ‰ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤! ê·¸ëƒ¥ ì“°ë©´ ë‚´ìš©ì´ ë¶€ì‹¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                draft = write_paper_final(target_section, ref_data)
                # ìƒì„±ëœ ë‚´ìš©ì„ í•´ë‹¹ ì±•í„° ì„œëì— ë„£ê¸°
                st.session_state['paper_sections'][target_section] = draft
                st.success("ìƒì„± ì™„ë£Œ!")

    # 3. ì—ë””í„° (ìƒì„±ëœ ê¸€ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ë³¼ ìˆ˜ ìˆëŠ” ê³³)
    st.markdown(f"### ğŸ“ {target_section} í¸ì§‘ê¸°")
    # ì„œëì—ì„œ êº¼ë‚´ì™€ì„œ ë³´ì—¬ì¤Œ
    current_text = st.text_area(
        label="ë‚´ìš©ì„ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        value=st.session_state['paper_sections'][target_section],
        height=500
    )
    
    # 4. ì €ì¥ ë²„íŠ¼
    if st.button(f"ğŸ’¾ {target_section} ë‚´ìš© ì €ì¥"):
        st.session_state['paper_sections'][target_section] = current_text
        st.success(f"{target_section} ë‚´ìš©ì´ ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# [Tab 5] ì°¸ê³ ë¬¸í—Œ
with tabs[4]:
    if st.button("APA ë³€í™˜"):
        st.markdown(organize_references_apa(st.session_state['research_context']['references']))

# -----------------------------------------------------------
# [Tab 6] AI ì‹¤ì‹œê°„ í”¼ë“œë°± (ëŒ€í™”í˜•)
# -----------------------------------------------------------
with tabs[5]:
    st.header("ğŸ’¬ AI ë…¼ë¬¸ ì§€ë„ êµìˆ˜ (í”¼ë“œë°± & ìˆ˜ì •)")
    st.info("4ë‹¨ê³„ì—ì„œ ì“´ ê¸€ì´ ë§ˆìŒì— ì•ˆ ë“¤ë©´ ì—¬ê¸°ì„œ ê³ ì³ë‹¬ë¼ê³  í•˜ì„¸ìš”. (ì˜ˆ: 'ì„œë¡ ì˜ ë…¼ë¦¬ì  ë¹„ì•½ì„ ìˆ˜ì •í•´ì¤˜')")

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ìˆ˜ì • ìš”ì²­ ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            # [ì¤‘ìš”] AIì—ê²Œ í˜„ì¬ê¹Œì§€ ì‘ì„±ëœ 'ëª¨ë“  ì±•í„°ì˜ ë‚´ìš©'ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
            current_paper_status = "\n".join([f"[{k}]: {v[:200]}..." for k, v in st.session_state['paper_sections'].items()])
            
            full_context = f"""
            [í˜„ì¬ ì—°êµ¬ ì§„í–‰ ìƒí™©]
            - ì£¼ì œ: {st.session_state['research_context']['topic']}
            - ë³€ì¸: {st.session_state['research_context']['variables']}
            - í˜„ì¬ ì‘ì„±ëœ ë…¼ë¬¸ ìš”ì•½:
            {current_paper_status}
            """
            
            # ì§€ë„êµìˆ˜ ëª¨ë“œ ë°œë™
            system_instruction = f"""
            ë‹¹ì‹ ì€ ê¹Œë‹¤ë¡œìš´ ì‹¬ë¦¬í•™ê³¼ ì§€ë„êµìˆ˜ì…ë‹ˆë‹¤.
            í•™ìƒ(ì‚¬ìš©ì)ì´ ë…¼ë¬¸ì˜ ë…¼ë¦¬ì  í—ˆì ì´ë‚˜ êµ¬ì²´ì„± ë¶€ì¡±ì„ ì§€ì í•˜ë©´, 
            1. ê·¸ ì§€ì ì´ íƒ€ë‹¹í•œì§€ í‰ê°€í•˜ê³ 
            2. êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ë¬¸ì¥ì„ í¬í•¨í•˜ì—¬ ì§ì ‘ ìˆ˜ì •ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
            3. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ì§€ë§Œ í•™ìˆ ì ìœ¼ë¡œ ì—„ê²©í•˜ê²Œ í•˜ì„¸ìš”.
            
            [ë°°ê²½ ì§€ì‹]: {full_context}
            """
            
            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "system", "content": system_instruction}] + 
                         [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
            )
            
            ai_response = response.choices[0].message.content
            message_placeholder.markdown(ai_response)
            st.session_state.messages.append({"role": "assistant", "content": ai_response})